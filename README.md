# Attention-in-Transformers-PyTorch
This project is a deep dive into the Attention Mechanism that powers modern Transformer architectures. It walks through the theory and implementation of self-attention, masked attention, encoder-decoder attention, and multi-headed attention â€” all built from scratch using PyTorch.
