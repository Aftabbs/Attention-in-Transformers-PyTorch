# ğŸ” Attention in Transformers: Concepts and Code in PyTorch

![image](https://github.com/user-attachments/assets/de259c0c-2da3-4a1a-a78e-fa9c2f0108cc)



This project is a deep dive into the **Attention Mechanism** that powers modern Transformer architectures. It walks through the theory and implementation of self-attention, masked attention, encoder-decoder attention, and multi-headed attention â€” all built from scratch using PyTorch.

## ğŸ“Œ Project Highlights

- âœ… **Custom implementation** of attention components in PyTorch
- ğŸ” **Self-Attention**: Foundation for understanding how tokens attend to each other
- ğŸš« **Masked Self-Attention**: Essential for autoregressive decoding in language models
- ğŸ”— **Encoder-Decoder Attention**: Facilitates input-output token alignment in sequence-to-sequence tasks
- ğŸ§  **Multi-Headed Attention**: Parallel attention heads for richer representation learning

---

## ğŸ”§ Core Implementations

### 1. Self-Attention
Implements the scaled dot-product attention:
```python
scores = Q @ K.T / sqrt(d_k)
weights = softmax(scores)
output = weights @ V
```

### 2. Masked Self-Attention
Applies causal mask to prevent attending to future tokens during decoding.

### 3. Encoder-Decoder Attention
Uses encoderâ€™s key-value with decoderâ€™s query to align relevant input context.

### 4. Multi-Head Attention
Projects input into multiple attention heads and aggregates the outputs:

---

## ğŸ§  Learning Objectives

- Understand the inner workings of attention mechanisms
- Reinforce theory through hands-on PyTorch code
- Prepare for building complete transformer architectures
- Build a foundation for advanced models like BERT, GPT, and T5

---

## ğŸ“˜ References

- [Attention is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)
- PyTorch Documentation


## ğŸš€ Next Steps

- ğŸ”„ Add positional encoding
- ğŸ— Build a full encoder-decoder Transformer
- ğŸ” Apply to NLP tasks like translation or summarization
